{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Sample_PR_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyhj9EojQjGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a293a29-c1b3-48e9-cb45-35acbd7c9c31"
      },
      "source": [
        "!pip install mtcnn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.7.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.19.5)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iunpQnSokweA"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import tensorflow as tf\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "import dlib\n",
        "import cv2 as cv\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torchvision.models.resnet as resnet\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzT66SkrwNv6",
        "outputId": "e23d1e19-1cbd-458d-b304-f521663f09bf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMNQIGln2nlU"
      },
      "source": [
        "!cp '/content/drive/MyDrive/checkpoint.pth.tar' -d '/content/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbh_LQBriDlU"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/Archive.zip' -d '/content/DAiSEE Dataset/DAiSEE/Dataset/Test'\n",
        "!unzip '/content/drive/MyDrive/Archive2.zip' -d '/content/DAiSEE Dataset/DAiSEE/Dataset/Test'\n",
        "!unzip '/content/drive/MyDrive/Archive3.zip' -d '/content/DAiSEE Dataset/DAiSEE/Dataset/Valid'\n",
        "!unzip '/content/drive/MyDrive/Archive4.zip' -d '/content/DAiSEE Dataset/DAiSEE/Dataset/Valid'\n",
        "!unzip '/content/drive/MyDrive/998826.zip' -d '/content/DAiSEE Dataset/DAiSEE/Dataset/Valid'\n",
        "# !mv \"/content/DAiSEE Dataset/DAiSEE/Dataset/Test/987736\" \"/content/DAiSEE Dataset/DAiSEE/Dataset/Valid\"\n",
        "# !mv \"/content/DAiSEE Dataset/DAiSEE/Dataset/Test/940328\" \"/content/DAiSEE Dataset/DAiSEE/Dataset/Valid\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTTicx2hlCio"
      },
      "source": [
        "# Split a video and save each frame\n",
        "def split_video(video_file, image_name_prefix, destination_path):\n",
        "    return subprocess.check_output('ffmpeg -i \"' + destination_path+video_file + '\" ' + image_name_prefix + '%d.jpg -hide_banner', shell=True, cwd=destination_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z45IjYxOsVTq"
      },
      "source": [
        "# Code for getting frame from each video\n",
        "\n",
        "for ttv in ['Test', 'Train', 'Validation']:\n",
        "    print('ttv is', ttv)\n",
        "    if ttv == '.DS_Store':\n",
        "        continue\n",
        "    users = os.listdir('DAiSEE Dataset/DAiSEE/Dataset/'+ttv+'/')\n",
        "    for user in users:\n",
        "        print('user is', user)\n",
        "        if user == '.DS_Store':\n",
        "            continue\n",
        "        currUser = os.listdir('DAiSEE Dataset/DAiSEE/Dataset/'+ttv+'/'+user+'/')\n",
        "        for extract in currUser:\n",
        "            print('extract is', extract)\n",
        "            if extract == '.DS_Store':\n",
        "                continue\n",
        "            clip = os.listdir('DAiSEE Dataset/DAiSEE/Dataset/'+ttv+'/'+user+'/'+extract+'/')[0]\n",
        "            if clip == '.DS_Store':\n",
        "                clip = os.listdir('DAiSEE Dataset/DAiSEE/Dataset/'+ttv+'/'+user+'/'+extract+'/')[1]\n",
        "            print (clip[:-4])\n",
        "            path = os.path.abspath('.')+'/DAiSEE Dataset/DAiSEE/Dataset/'+ttv+'/'+user+'/'+extract+'/'\n",
        "            split_video(clip, clip[:-4], path)\n",
        "            # break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EvQ8iX_tPrU"
      },
      "source": [
        "# Move each frame into a separate directory\n",
        "\n",
        "for x in ['Test', 'Valid']:\n",
        "  for i in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x):\n",
        "    if i not in ['__MACOSX','.ipynb_checkpoints']:\n",
        "      for j in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'/'+i):\n",
        "        if j == '.DS_Store':\n",
        "          continue\n",
        "        for k in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'/'+i+'/'+j):\n",
        "          if k[-4:] == '.jpg':\n",
        "            try:\n",
        "              shutil.move('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'/'+i+'/'+j+'/'+k, '/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'Frames/'+i+'/'+j+'/')\n",
        "            except IOError as io_err:\n",
        "              os.makedirs(os.path.dirname('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'Frames/'+i+'/'+j+'/'))\n",
        "              shutil.move('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'/'+i+'/'+j+'/'+k, '/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'Frames/'+i+'/'+j+'/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klfnzH9TYNMc"
      },
      "source": [
        "# !rm -rf '/content/DAiSEE Dataset/DAiSEE/Dataset/Train/'\n",
        "# !rm -rf '/content/DAiSEE Dataset/DAiSEE/Dataset/Test/'\n",
        "# !rm -rf '/content/DAiSEE Dataset/DAiSEE/Dataset/Valid/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJkvOBtOeT-6"
      },
      "source": [
        "# !rm -rf '/content/DAiSEE Dataset/DAiSEE/Dataset2/TestFaces/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZYQbHcXswkr"
      },
      "source": [
        "# Extract a single face from a given photograph\n",
        "\n",
        "def extract_face(filename, required_size=(160, 160)):\n",
        "    image = Image.open(filename)\n",
        "    image = image.convert('RGB')\n",
        "    pixels = np.asarray(image)\n",
        "    detector = MTCNN()\n",
        "    # detect faces in the image\n",
        "    results = detector.detect_faces(pixels)\n",
        "    x1, y1, width, height = results[0]['box']\n",
        "    x1, y1 = abs(x1), abs(y1)\n",
        "    x2, y2 = x1 + width, y1 + height\n",
        "    # extract the face\n",
        "    face = pixels[y1:y2, x1:x2]\n",
        "    image = Image.fromarray(face)\n",
        "    image = image.resize(required_size)\n",
        "    face_array = np.asarray(image)\n",
        "    return face_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcsHB8_2s0gE"
      },
      "source": [
        "# Run the FaceNet model on each video on different parts of the 10-sec video\n",
        "\n",
        "cnt = 0\n",
        "for x in ['TestFrames','ValidFrames']:\n",
        "  for i in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x):\n",
        "    if i not in ['__MACOSX','.ipynb_checkpoints']:\n",
        "      for j in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'/'+i):\n",
        "        if j == '.DS_Store':\n",
        "          continue\n",
        "        print('At file', j)\n",
        "        cnt = 0\n",
        "        for k in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'/'+i+'/'+j):\n",
        "          cnt = cnt+1\n",
        "          if cnt in [15,135,285]:\n",
        "            pixels = extract_face('/content/DAiSEE Dataset/DAiSEE/Dataset/'+x+'/'+i+'/'+j+'/'+k)\n",
        "            im = Image.fromarray(pixels)\n",
        "            try:\n",
        "              im.save('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x[0:-6]+'Faces/'+i+'/'+j+'/'+k)\n",
        "            except IOError as io_err:\n",
        "              os.makedirs(os.path.dirname('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x[0:-6]+'Faces/'+i+'/'+j+'/'))\n",
        "              im.save('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x[0:-6]+'Faces/'+i+'/'+j+'/'+k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JnZNY0Xfrwx"
      },
      "source": [
        "!unzip '/content/drive/MyDrive/TrainFaces.zip'\n",
        "!unzip '/content/drive/MyDrive/TestFaces.zip'\n",
        "!unzip '/content/drive/MyDrive/ValidFaces.zip'\n",
        "\n",
        "!mv \"/content/content/DAiSEE Dataset/DAiSEE/Dataset/TestFaces\" \"/content/DAiSEE Dataset/DAiSEE/Dataset2\"\n",
        "!mv \"/content/content/DAiSEE Dataset/DAiSEE/Dataset/TrainFaces\" \"/content/DAiSEE Dataset/DAiSEE/Dataset2\"\n",
        "!mv \"/content/content/DAiSEE Dataset/DAiSEE/Dataset/ValidFaces\" \"/content/DAiSEE Dataset/DAiSEE/Dataset2\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hANkKM3Vu5HH"
      },
      "source": [
        "# This file is part of EmotionNet2 a system for predicting facial emotions\n",
        "# Copyright (C) 2018  Maeve Kennedy\n",
        "# \n",
        "# This program is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "# \n",
        "# This program is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
        "# GNU General Public License for more details.\n",
        "# \n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with this program.  If not, see <https://www.gnu.org/licenses/>\n",
        "\n",
        "import torchvision.models.resnet as resnet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.autograd import Variable\n",
        "import shutil\n",
        "# For windowing\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# For one file pass\n",
        "import tempfile\n",
        "# import face_detector\n",
        "import os.path\n",
        "from PIL import Image\n",
        "# import extract_faces\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.DEBUG)\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class Confusion():\n",
        "    def __init__(self, loader):\n",
        "        self.classes = loader.dataset.classes\n",
        "        self.class_to_idx = loader.dataset.class_to_idx\n",
        "        self.count = np.zeros((len(self.classes), len(self.classes)))\n",
        "        # Rows are inputs, cols are outputs\n",
        "\n",
        "    def add(self, output, input):\n",
        "        output = output.cpu().data.numpy()\n",
        "        input = input.cpu().numpy()\n",
        "        for i, o in enumerate(np.argmax(output, axis=1)):\n",
        "            self.count[input[i], o] += 1\n",
        "\n",
        "    def print_confusion(self):\n",
        "        out = np.copy(self.count)\n",
        "        col_sums = out.sum(axis=1)\n",
        "        out = out / col_sums[:, np.newaxis]\n",
        "        print(out)\n",
        "\n",
        "    def print_latex(self, avg):\n",
        "        out = np.copy(self.count)\n",
        "        col_sums = out.sum(axis=1)\n",
        "        out = out / col_sums[:, np.newaxis]\n",
        "        # Print LaTeX style\n",
        "        print('\\\\begin{tabular}{' + 'l'*(len(self.classes)+1) + '}')\n",
        "        print('\\\\hline')\n",
        "        print('\\t & ', end='')\n",
        "        print(*self.classes, sep=' & ', end='\\\\\\\\\\n')\n",
        "        print('\\\\hline')\n",
        "        for ri, r in enumerate(out):\n",
        "            print('\\t{} & '.format(self.classes[ri]), end='')\n",
        "            for ci, c in enumerate(r):\n",
        "                print('{0:.3f} {1}'.format(c * 100, '& ' if ci != len(r)-1 else '\\\\\\\\'),\n",
        "                     end=('' if ci != len(r)-1 else '\\n'))\n",
        "        print('\\\\hline')\n",
        "        print('\\\\end{tabular} \\\\\\\\')\n",
        "        print('Average: {:.3}\\\\%'.format(avg))\n",
        "                \n",
        "\n",
        "class EmotionNet():\n",
        "    def __init__(self, layers=[3, 4, 6, 3]):\n",
        "        block = resnet.BasicBlock\n",
        "        num_classes = 4\n",
        "        self.model = resnet.ResNet(block, layers, num_classes)\n",
        "        if torch.cuda.is_available():\n",
        "            self.model.cuda()\n",
        "        self.bestaccur = 0.0\n",
        "\n",
        "    def save_checkpoint(self, is_best, filename='checkpoint.pth.tar'):\n",
        "        torch.save(self.model.state_dict(), filename)\n",
        "        if is_best:\n",
        "            shutil.copyfile(filename, 'best-' + filename)\n",
        "\n",
        "    def load_checkpoint(self, filename):\n",
        "        sample_model = resnet.ResNet(resnet.BasicBlock, [3, 4, 23, 3], 7)\n",
        "        if torch.cuda.is_available():\n",
        "            sample_model.load_state_dict(torch.load(filename))\n",
        "        else:\n",
        "            sample_model.load_state_dict(torch.load(filename, map_location=torch.device('cpu')))\n",
        "        sample_model.fc = nn.Linear(512, 4)\n",
        "        self.model.load_state_dict(sample_model.state_dict())\n",
        "    def test_model_show(self, testdir, show=False):\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "        testdata = datasets.ImageFolder(testdir, transforms.Compose([\n",
        "            transforms.Scale(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "        ]))\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        if torch.cuda.is_available():\n",
        "            criterion = criterion.cuda()\n",
        "        # Test\n",
        "        self.model.eval()\n",
        "        losses = AverageMeter()\n",
        "        top1 = AverageMeter()\n",
        "        for i, (input, target) in enumerate(testdata):\n",
        "            img = np.copy(input.numpy())\n",
        "            target_out = torch.LongTensor(1, 1).zero_()\n",
        "            target_out[0][0] = target\n",
        "            if torch.cuda.is_available():\n",
        "                target_out = target_out.cuda(non_blocking=True)\n",
        "            input_var = Variable(normalize(input).view(1, *input.size()), volatile=True)\n",
        "            if torch.cuda.is_available():\n",
        "                input_var = input_var.cuda()\n",
        "\n",
        "            output = self.model(input_var)\n",
        "            prec1 = accuracy(output.data, target_out, topk=(1,))\n",
        "            top1.update(prec1[0][0], input.size(0))\n",
        "            img = np.rollaxis(img, 0, 3)\n",
        "            plt.title(prec1[0].cpu().numpy()[0])\n",
        "            plt.imshow(img)\n",
        "            plt.pause(0.01)\n",
        "            \n",
        "        print('Test, Prec: {}'.format(\n",
        "              top1.avg))\n",
        "\n",
        "    def train_model(self, datadir, validdir, outprefix, epochs=10, csvout=None):\n",
        "        batch_size = 2\n",
        "        num_workers = 4\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            datasets.ImageFolder(datadir, transforms.Compose([\n",
        "                transforms.RandomSizedCrop(224),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])),\n",
        "            batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        valid_loader = torch.utils.data.DataLoader(\n",
        "            datasets.ImageFolder(validdir, transforms.Compose([\n",
        "                transforms.Scale(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])),\n",
        "            batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        if torch.cuda.is_available():\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        # CSV\n",
        "        if csvout is not None:\n",
        "            csvf = open(csvout, 'w')\n",
        "\n",
        "        # Switch to training\n",
        "        for e in range(epochs):\n",
        "            self.model.train()\n",
        "            losses = AverageMeter()\n",
        "            top1 = AverageMeter()\n",
        "            # Train\n",
        "            for i, (input, target) in enumerate(train_loader):\n",
        "                if torch.cuda.is_available():\n",
        "                    target = target.cuda(non_blocking=True)\n",
        "                    input_var = Variable(input).cuda()\n",
        "                else:\n",
        "                    input_var = Variable(input)\n",
        "                target_var = Variable(target)\n",
        "                output = self.model(input_var)\n",
        "\n",
        "                loss = criterion(output, target_var)\n",
        "                prec1 = accuracy(output.data, target, topk=(1,))\n",
        "                losses.update(loss.data, input.size(0))\n",
        "                top1.update(prec1[0][0], input.size(0))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if i % 5 == 0:\n",
        "                    logging.info('Epoch: {} {}/{}\\nLoss: {}\\nPrec: {}'.format(\n",
        "                      e, i, len(train_loader), losses.avg, top1.avg))\n",
        "            # Valid\n",
        "            lossavg, topavg = self._valid_model(valid_loader)\n",
        "                \n",
        "            logging.info('Epoch: {}, Validation Loss: {}, Prec: {}'.format(\n",
        "                  e, lossavg, topavg))\n",
        "            if csvout is not None:\n",
        "                # Epoch, training loss, training accuracy, valid lossavg, topavg\n",
        "                print('{}, {}, {}, {}, {}'.format(e, losses.avg, top1.avg, \n",
        "                      lossavg, topavg), flush=True, file=csvf)\n",
        "            top = False\n",
        "            if self.bestaccur < top1.avg:\n",
        "                self.bestaccur = top1.avg\n",
        "                top = True\n",
        "            self.save_checkpoint(top, filename=outprefix + '.pth.tar')\n",
        "        if csvout is not None:\n",
        "            csvf.close()\n",
        "    \n",
        "    def valid_model(self, validdir):\n",
        "        batch_size = 16\n",
        "        num_workers = 4\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "\n",
        "        valid_loader = torch.utils.data.DataLoader(\n",
        "            datasets.ImageFolder(validdir, transforms.Compose([\n",
        "                transforms.Scale(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ])),\n",
        "            batch_size=batch_size, shuffle=False, num_workers=num_workers,\n",
        "            pin_memory=torch.cuda.is_available()\n",
        "        )\n",
        "\n",
        "        loss, acc = self._valid_model(valid_loader)\n",
        "        logging.info('Loss: {}, Precision: {}'.format(loss, acc))\n",
        "    \n",
        "    def classify_one_image(self, imgf,\n",
        "            classes=['afraid', 'angry', 'disgusted', 'happy', 'neutral', 'sad', 'surprised']):\n",
        "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                         std=[0.229, 0.224, 0.225])\n",
        "        transf = transforms.Compose([\n",
        "                transforms.Scale(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "        ])\n",
        "        # Face detection\n",
        "        args = {}\n",
        "        args['threshold'] = 0.0\n",
        "        args['window'] = False\n",
        "        args['ignore_multi'] = True\n",
        "        args['grow'] = 10 \n",
        "        args['resize'] = True\n",
        "        args['row_resize'] = 512\n",
        "        args['col_resize'] = 512\n",
        "        args['min_proportion'] = 0.1\n",
        "        \n",
        "        with tempfile.TemporaryDirectory() as tempdir:\n",
        "            args['o'] = tempdir\n",
        "            # face_detector.transform(extract_faces.AttributeDict(args), [imgf])\n",
        "            # cropped = Image.open(tempdir + '/' + os.path.basename(imgf))\n",
        "        cropped = Image.open(imgf)\n",
        "        cropped = transf(cropped)\n",
        "        \n",
        "        input_var = Variable(cropped.view(1, *cropped.shape))\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            input_var = input_var.cuda()\n",
        "\n",
        "        output = self.model.forward(input_var).cpu().data.numpy()\n",
        "        softmax = np.exp(output) / np.sum(np.exp(output))\n",
        "        clss = np.argmax(softmax)\n",
        "        # print('clss is', clss)\n",
        "        # print('softmax is', softmax)\n",
        "        # print('max of softmax is', softmax.max())\n",
        "        return clss, softmax.max()\n",
        "        # fig = plt.figure()\n",
        "        # plt.imshow(Image.open(imgf))\n",
        "        # fig.subplots_adjust(bottom=0.2)\n",
        "        # plt.figtext(0.1, 0.05, ', '.join(classes))\n",
        "        # plt.figtext(0.1, 0.10, ', '.join(['{:.3}'.format(a) for a in softmax.reshape(-1)]))\n",
        "        # plt.title(classes[clss])\n",
        "        # plt.show()\n",
        "\n",
        "    def _valid_model(self, valid_loader): \n",
        "        self.model.eval()\n",
        "        losses = AverageMeter()\n",
        "        top1 = AverageMeter()\n",
        "        confusion = Confusion(valid_loader)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "        for i, (input, target) in enumerate(valid_loader):\n",
        "            if torch.cuda.is_available():\n",
        "                target = target.cuda(non_blocking=True)\n",
        "            input_var = Variable(input, volatile=True)\n",
        "            if torch.cuda.is_available():\n",
        "                input_var = input_var.cuda()\n",
        "            target_var = Variable(target, volatile=True)\n",
        "\n",
        "            output = self.model(input_var)\n",
        "            confusion.add(output, target)\n",
        "            loss = criterion(output, target_var)\n",
        "\n",
        "            prec1 = accuracy(output.data, target, topk=(1,))\n",
        "            losses.update(loss.data, input.size(0))\n",
        "            top1.update(prec1[0][0], input.size(0))\n",
        "        print('Confusion Matrix')\n",
        "        confusion.print_confusion()\n",
        "        confusion.print_latex(top1.avg)\n",
        "        return losses.avg, top1.avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYUkym7_pM9G",
        "outputId": "e106880f-b63e-4b9f-daaa-642238ae40ea"
      },
      "source": [
        "net = EmotionNet(layers=[3, 4, 23, 3])\n",
        "net.load_checkpoint('/content/checkpoint.pth.tar')\n",
        "\n",
        "net.classify_one_image('/content/DAiSEE Dataset/DAiSEE/Dataset2/TrainFaces/510035/5100351001/5100351001112.jpg', classes=['boredom-0', 'boredom-1', 'boredom-2', 'boredom-3'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:317: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 0.298013)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZZeJXBZyZk"
      },
      "source": [
        "results_frame = pd.DataFrame(columns= ['Video ID', 'Frame ID', 'Boredom class', 'Boredom prob', 'Engagement class', 'Engagement prob', 'Confusion class', 'Confusion prob', 'Frustration class', 'Frustration prob'])\n",
        "\n",
        "net = EmotionNet(layers=[3, 4, 23, 3])\n",
        "net.load_checkpoint('checkpoint.pth.tar')\n",
        "\n",
        "for x in ['TrainFaces', 'TestFaces', 'ValidFaces']:\n",
        "  for i in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x):\n",
        "    if i not in ['__MACOSX','.ipynb_checkpoints']:\n",
        "      for j in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x+'/'+i):\n",
        "        if j == '.DS_Store':\n",
        "          continue\n",
        "        print('At file', j)\n",
        "        for k in os.listdir('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x+'/'+i+'/'+j):\n",
        "          b_class, b_prob = net.classify_one_image('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x+'/'+i+'/'+j+'/'+k, classes=['boredom-0', 'boredom-1', 'boredom-2', 'boredom-3'])\n",
        "          e_class, e_prob = net.classify_one_image('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x+'/'+i+'/'+j+'/'+k, classes=['engagement-0', 'engagement-1', 'engagement-2', 'engagement-3'])\n",
        "          c_class, c_prob = net.classify_one_image('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x+'/'+i+'/'+j+'/'+k, classes=['confusion-0', 'confusion-1', 'confusion-2', 'confusion-3'])\n",
        "          f_class, f_prob = net.classify_one_image('/content/DAiSEE Dataset/DAiSEE/Dataset2/'+x+'/'+i+'/'+j+'/'+k, classes=['frustration-0', 'frustration-1', 'frustration-2', 'frustration-3'])\n",
        "          results_frame = results_frame.append({\n",
        "              'Video ID': j,\n",
        "              'Frame ID': k[0:-4],\n",
        "              'Boredom class': b_class,\n",
        "              'Boredom prob': b_prob,\n",
        "              'Engagement class': e_class,\n",
        "              'Engagement prob': e_prob,\n",
        "              'Confusion class': c_class,\n",
        "              'Confusion prob': c_prob,\n",
        "              'Frustration class': f_class,\n",
        "              'Frustration prob': f_prob},\n",
        "              ignore_index = True)\n",
        "# classify_one_image('/content/DAiSEE Dataset/DAiSEE/Dataset/Test/510035/5100351001/51003510011.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyJeFVuOcR2z"
      },
      "source": [
        "# results_frame.to_csv('/content/results_frame.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X47Ef8TFO3yS"
      },
      "source": [
        "results_frame = pd.read_csv('/content/results_frame.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IY2__9TsBhL",
        "outputId": "795aa46b-1c7c-4ba3-9da9-042edce16db2"
      },
      "source": [
        "results_frame['Video ID'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "239"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xq5YMVqisBj3"
      },
      "source": [
        "results = results_frame.groupby(['Video ID']).agg({\n",
        "    'Boredom class': lambda x: x.mode().iloc[0],\n",
        "    'Engagement class': lambda x: x.mode().iloc[0],\n",
        "    'Confusion class': lambda x: x.mode().iloc[0],\n",
        "    'Frustration class': lambda x: x.mode().iloc[0]\n",
        "}).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "NNSz-KgcRc7Z",
        "outputId": "c16f4120-3b7e-4980-afe2-2ec6a03d0968"
      },
      "source": [
        "results_frame.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Video ID</th>\n",
              "      <th>Frame ID</th>\n",
              "      <th>Boredom class</th>\n",
              "      <th>Boredom prob</th>\n",
              "      <th>Engagement class</th>\n",
              "      <th>Engagement prob</th>\n",
              "      <th>Confusion class</th>\n",
              "      <th>Confusion prob</th>\n",
              "      <th>Frustration class</th>\n",
              "      <th>Frustration prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>5100351010</td>\n",
              "      <td>5100351010133</td>\n",
              "      <td>2</td>\n",
              "      <td>0.288267</td>\n",
              "      <td>2</td>\n",
              "      <td>0.288267</td>\n",
              "      <td>2</td>\n",
              "      <td>0.288267</td>\n",
              "      <td>2</td>\n",
              "      <td>0.288267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5100351010</td>\n",
              "      <td>5100351010258</td>\n",
              "      <td>2</td>\n",
              "      <td>0.280239</td>\n",
              "      <td>2</td>\n",
              "      <td>0.280239</td>\n",
              "      <td>2</td>\n",
              "      <td>0.280239</td>\n",
              "      <td>2</td>\n",
              "      <td>0.280239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>5100351010</td>\n",
              "      <td>5100351010150</td>\n",
              "      <td>2</td>\n",
              "      <td>0.275746</td>\n",
              "      <td>2</td>\n",
              "      <td>0.275746</td>\n",
              "      <td>2</td>\n",
              "      <td>0.275746</td>\n",
              "      <td>2</td>\n",
              "      <td>0.275746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>5100351010</td>\n",
              "      <td>5100351010221</td>\n",
              "      <td>2</td>\n",
              "      <td>0.293650</td>\n",
              "      <td>2</td>\n",
              "      <td>0.293650</td>\n",
              "      <td>2</td>\n",
              "      <td>0.293650</td>\n",
              "      <td>2</td>\n",
              "      <td>0.293650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>5100351010</td>\n",
              "      <td>510035101032</td>\n",
              "      <td>2</td>\n",
              "      <td>0.277615</td>\n",
              "      <td>2</td>\n",
              "      <td>0.277615</td>\n",
              "      <td>2</td>\n",
              "      <td>0.277615</td>\n",
              "      <td>2</td>\n",
              "      <td>0.277615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0    Video ID  ...  Frustration class  Frustration prob\n",
              "0           0  5100351010  ...                  2          0.288267\n",
              "1           1  5100351010  ...                  2          0.280239\n",
              "2           2  5100351010  ...                  2          0.275746\n",
              "3           3  5100351010  ...                  2          0.293650\n",
              "4           4  5100351010  ...                  2          0.277615\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOw7nOSFsBmd"
      },
      "source": [
        "actual_labels = pd.read_csv('/content/drive/MyDrive/TestLabels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6A6Jsg8PpjQ",
        "outputId": "80c38baf-6630-498c-a5ab-fdb293b781a6"
      },
      "source": [
        "results.dtypes, actual_labels.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Video ID             int64\n",
              " Boredom class        int64\n",
              " Engagement class     int64\n",
              " Confusion class      int64\n",
              " Frustration class    int64\n",
              " dtype: object, ClipID          object\n",
              " Boredom          int64\n",
              " Engagement       int64\n",
              " Confusion        int64\n",
              " Frustration      int64\n",
              " dtype: object)"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lHY_k4VsBoz"
      },
      "source": [
        "results['Video ID'] = results['Video ID'].astype(str)\n",
        "actual_labels['ClipID'] = actual_labels['ClipID'].str[0:-4]\n",
        "results_comparison = pd.merge(results, actual_labels, left_on = 'Video ID', right_on = 'ClipID', how = 'left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTgR_u2pR2Lw"
      },
      "source": [
        "def top1accu(df, col1, col2):\n",
        "  count = 0\n",
        "  for index, row in df.iterrows():\n",
        "    if row[col1] == row[col2]:\n",
        "      count = count + 1\n",
        "  return count*100/df.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meJIuJeNScYD",
        "outputId": "8626420a-b770-4d4e-fd31-ff24b66cacc1"
      },
      "source": [
        "print('Boredom Accuracy is ',top1accu(results_comparison, 'Boredom class', 'Boredom'))\n",
        "print('Engagement Accuracy is ',top1accu(results_comparison, 'Engagement class', 'Engagement'))\n",
        "print('Confusion Accuracy is ',top1accu(results_comparison, 'Confusion class', 'Confusion'))\n",
        "print('Frustration Accuracy is ',top1accu(results_comparison, 'Frustration class', 'Frustration '))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boredom Accuracy is  18.828451882845187\n",
            "Engagement Accuracy is  51.88284518828452\n",
            "Confusion Accuracy is  5.439330543933054\n",
            "Frustration Accuracy is  2.092050209205021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0YDAEZqScBv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}